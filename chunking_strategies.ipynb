{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfSsgxXlmm3g3sSyYNPlWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna324-art/RAG---Vanilla/blob/main/chunking_strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNME4zWHT2oW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpngD357ds0k",
        "outputId": "3cc1b0ff-ccc2-45d4-f52e-278a7f093e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running in Google Colab, installing requirements.\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.6\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.9.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n",
            "Failed to build flash-attn\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    #!pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "   # !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-36Retxfmi8",
        "outputId": "05d20741-4312-4847-9452-016b766817dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.9.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n",
            "Failed to build flash-attn\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download PDF file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Get PDF document\n",
        "pdf_path = \"human-nutrition-text.pdf\"\n",
        "\n",
        "# Download PDF if it doesn't already exist\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "  # The URL of the PDF you want to download\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "\n",
        "  # The local filename to save the downloaded file\n",
        "  filename = pdf_path\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Open a file in binary write mode and save the content to it\n",
        "      with open(filename, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"The file has been downloaded and saved as {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "  print(f\"File {pdf_path} exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBkSQdQxeFDe",
        "outputId": "ddbdc6ed-9b58-4120-a2d5-11e43d96ecdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File human-nutrition-text.pdf exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
        "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Performs minor formatting on text.\"\"\"\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
        "\n",
        "    # Other potential text formatting functions can go here\n",
        "    return cleaned_text\n",
        "\n",
        "# Open PDF and get lines/pages\n",
        "# Note: this only focuses on text, rather than images/figures etc\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, each containing the page number\n",
        "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
        "        for each page.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)  # open a document\n",
        "    pages_and_texts = []\n",
        "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
        "        text = page.get_text()  # get plain text encoded as UTF-8\n",
        "        text = text_formatter(text)\n",
        "        pages_and_texts.append({\"page_number\": page_number - 41,\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                                \"page_token_count\": len(text) / 4,\n",
        "                                \"text\": text})\n",
        "    return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "29734fdb863a4840b7d53ddac7ec016d",
            "ff71ff956c2c44b5a6b9ff8f56ce163d",
            "dcdd838ebe804656aea4fcf49f874f27",
            "a6567e971ed04fe9864b4fd9187efe14",
            "afa109dedccc4f05b4e16057bf30d737",
            "1ccc9da08d824714a81483c6a5ff6632",
            "bf515779ee324a14a6a5c60780e4da77",
            "8453892df75e4b88b6110a08f558d234",
            "9665a489f8c7478f96c36d2a86b625ac",
            "8e8164783b104e12b24b891c09da4e86",
            "4122b95f7238461e9e37d0ae4ff9a3b8"
          ]
        },
        "id": "aXBexEc7etJc",
        "outputId": "25aeb869-8df6-40a8-acfa-bde21ff7f734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29734fdb863a4840b7d53ddac7ec016d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': -41,\n",
              "  'page_char_count': 29,\n",
              "  'page_word_count': 4,\n",
              "  'page_sentence_count_raw': 1,\n",
              "  'page_token_count': 7.25,\n",
              "  'text': 'Human Nutrition: 2020 Edition'},\n",
              " {'page_number': -40,\n",
              "  'page_char_count': 0,\n",
              "  'page_word_count': 1,\n",
              "  'page_sentence_count_raw': 1,\n",
              "  'page_token_count': 0.0,\n",
              "  'text': ''}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int = 500) -> list:\n",
        "    \"\"\"\n",
        "    Splits text into chunks of approx. `chunk_size` characters.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = ''\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        # Check if adding the word exceeds chunk size\n",
        "        if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
        "            current_chunk += (word + ' ')\n",
        "        else:\n",
        "            # Store current chunk and start new one\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = word + ' '\n",
        "\n",
        "    # Add the last chunk if not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_pdf_pages(pages_and_texts: list, chunk_size: int = 500) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Takes PDF pages with text and splits them into chunks.\n",
        "\n",
        "    Returns a list of dicts with page_number, chunk_index, and chunk_text.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    for page in pages_and_texts:\n",
        "        page_number = page[\"page_number\"]\n",
        "        page_text = page[\"text\"]\n",
        "\n",
        "        chunks = chunk_text(page_text, chunk_size=chunk_size)\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"page_number\": page_number,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_char_count\": len(chunk),\n",
        "                \"chunk_word_count\": len(chunk.split()),\n",
        "                \"chunk_token_count\": len(chunk) / 4,  # rough token estimate\n",
        "                \"chunk_text\": chunk\n",
        "            })\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "# Example usage\n",
        "chunked_pages = chunk_pdf_pages(pages_and_texts, chunk_size=500)\n",
        "print(f\"Total chunks: {len(chunked_pages)}\")\n",
        "print(f\"First chunk (page {chunked_pages[0]['page_number']}): {chunked_pages[0]['chunk_text'][:200]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfM7VTRvex3A",
        "outputId": "814496b7-d99b-4b2e-a067-efeccbf567fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 3321\n",
            "First chunk (page -41): Human Nutrition: 2020 Edition...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random, textwrap\n",
        "\n",
        "# ---------- Sampling & Pretty Printing ----------\n",
        "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
        "    \"\"\"Evenly spaced anchors + random jitter → indices scattered across [0, n-1].\"\"\"\n",
        "    if k <= 0:\n",
        "        return []\n",
        "    if k == 1:\n",
        "        return [random.randrange(n)]\n",
        "    anchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "    out, seen = [], set()\n",
        "    radius = max(1, int(n * jitter_frac))\n",
        "    for a in anchors:\n",
        "        lo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
        "        j = random.randint(lo, hi)\n",
        "        if j not in seen:\n",
        "            out.append(j); seen.add(j)\n",
        "    while len(out) < k:\n",
        "        r = random.randrange(n)\n",
        "        if r not in seen:\n",
        "            out.append(r); seen.add(r)\n",
        "    return out\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    header = (\n",
        "        f\" Chunk p{c['page_number']} · idx {c['chunk_index']}  |  \"\n",
        "        f\"chars {c['chunk_char_count']} · words {c['chunk_word_count']} · ~tokens {c['chunk_token_count']} \"\n",
        "    )\n",
        "    # Wrap body text, avoid breaking long words awkwardly\n",
        "    wrapped_lines = textwrap.wrap(\n",
        "        c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "    )\n",
        "    content_width = max([0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), content_width + 2)  # +2 for side padding\n",
        "\n",
        "    top    = \"╔\" + \"═\" * box_width + \"╗\"\n",
        "    hline  = \"║\" + header.ljust(box_width) + \"║\"\n",
        "    sep    = \"╟\" + \"─\" * box_width + \"╢\"\n",
        "    body   = \"\\n\".join(\"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or \\\n",
        "             (\"║ \" + \"\".ljust(box_width - 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"═\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "\n",
        "def show_random_chunks(pages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "    all_chunks = chunk_pdf_pages(pages_and_texts, chunk_size=chunk_size)\n",
        "    if not all_chunks:\n",
        "        print(\"No chunks to display.\");\n",
        "        return\n",
        "    idxs = _scattered_indices(len(all_chunks), k)\n",
        "    print(f\"Showing {len(idxs)} scattered random chunks out of {len(all_chunks)} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(all_chunks[idx]))\n",
        "        print()\n",
        "\n",
        "# ---------- Run ----------\n",
        "assert 'pages_and_texts' in globals(), \"Run: pages_and_texts = open_and_read_pdf(pdf_path) first.\"\n",
        "show_random_chunks(pages_and_texts, chunk_size=500, k=5, seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvSGeA64hHEC",
        "outputId": "96991b93-2467-44a7-de53-0021da31a7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing 5 scattered random chunks out of 3321 total:\n",
            "\n",
            "#1\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p-9 · idx 0  |  chars 290 · words 49 · ~tokens 72.5                                        ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Skylar Hara Skylar Hara is an undergraduate student student in the Tropical Agriculture and the  ║\n",
            "║ Environment program at the University of Hawai‘i at Mānoa. She has a growing love for plants and ║\n",
            "║ hopes to go to graduate school to conduct research in the future. About the Contributors |       ║\n",
            "║ xxxiii                                                                                           ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#2\n",
            "╔═════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p198 · idx 0  |  chars 497 · words 71 · ~tokens 124.25                                    ║\n",
            "╟─────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ foods. Fresh and frozen foods are better sources of potassium than canned. Learning Activities  ║\n",
            "║ Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER)      ║\n",
            "║ textbook features interactive learning activities. These activities are available in the web-   ║\n",
            "║ based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or ║\n",
            "║ Open Document). Learning activities may be used across various mobile devices, however, for the ║\n",
            "║ best user experience it is                                                                      ║\n",
            "╚═════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#3\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p579 · idx 0  |  chars 496 · words 77 · ~tokens 124.0                                      ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Image by Allison Calabrese / CC BY 4.0 Folate is especially essential for the growth and         ║\n",
            "║ specialization of cells of the central nervous system. Children whose mothers were folate-       ║\n",
            "║ deficient during pregnancy have a higher risk of neural-tube birth defects. Folate deficiency is ║\n",
            "║ causally linked to the development of spina bifida, a neural-tube defect that occurs when the    ║\n",
            "║ spine does not completely enclose the spinal cord. Spina bifida can lead to many physical and    ║\n",
            "║ mental disabilities (Figure 9.18                                                                 ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#4\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p882 · idx 2  |  chars 300 · words 34 · ~tokens 75.0                                       ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ best user experience it is strongly 9. Lead Exposure: Tips to Protect Your Child. Mayo           ║\n",
            "║ Foundation for Medical Education and Research. https://www.mayoclinic.org/diseases-              ║\n",
            "║ conditions/lead- poisoning/in-depth/lead-exposure/art-20044627. Updated March 12, 2015. Accessed ║\n",
            "║ December 5, 2017. 882 | Childhood                                                                ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#5\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p1117 · idx 1  |  chars 456 · words 57 · ~tokens 114.0                                     ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ triglycerides greater than 150 mg/dL; high density lipoproteins (HDL) lower than 40 mg/dL;       ║\n",
            "║ systolic blood pressure above 100 mmHg, or diastolic above 85 mmHg; fasting blood-glucose levels ║\n",
            "║ greater than 100 mg/dL.16 The IDF estimates that between 20 and 16. The IDF Consensus Worldwide  ║\n",
            "║ Definition of the Metabolic Syndrome. International Diabetes Federation.https://www.idf.org/our- ║\n",
            "║ activities/ advocacy-awareness/resources-and-tools/ Threats to Health | 1117                     ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#semantic chunking\n",
        "!pip -q install --upgrade \"sentence-transformers==3.0.1\" \"transformers<5,>=4.41\" scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kvt2pc9jRIG",
        "outputId": "acd4fa7f-e29b-47bb-875f-b4d35d25c6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Load once globally\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def semantic_chunk_text(text: str, similarity_threshold: float = 0.8, max_tokens: int = 500) -> list:\n",
        "    \"\"\"\n",
        "    Splits text into semantic chunks based on sentence similarity and max token length.\n",
        "    \"\"\"\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    embeddings = semantic_model.encode(sentences)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "    current_embedding = embeddings[0]\n",
        "\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = cosine_similarity([current_embedding], [embeddings[i]])[0][0]\n",
        "        chunk_token_count = len(\" \".join(current_chunk)) // 4\n",
        "\n",
        "        if sim >= similarity_threshold and chunk_token_count < max_tokens:\n",
        "            current_chunk.append(sentences[i])\n",
        "            current_embedding = np.mean([current_embedding, embeddings[i]], axis=0)\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentences[i]]\n",
        "            current_embedding = embeddings[i]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def semantic_chunk_pdf_pages(pages_and_texts: list,\n",
        "                             similarity_threshold: float = 0.8,\n",
        "                             max_tokens: int = 500) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Takes PDF pages with text and splits them into semantic chunks.\n",
        "\n",
        "    Returns a list of dicts with page_number, chunk_index, and chunk_text.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    for page in tqdm(pages_and_texts, desc=\"Semantic chunking pages\"):\n",
        "        page_number = page[\"page_number\"]\n",
        "        page_text = page[\"text\"]\n",
        "\n",
        "        chunks = semantic_chunk_text(page_text,\n",
        "                                     similarity_threshold=similarity_threshold,\n",
        "                                     max_tokens=max_tokens)\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"page_number\": page_number,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_char_count\": len(chunk),\n",
        "                \"chunk_word_count\": len(chunk.split()),\n",
        "                \"chunk_token_count\": len(chunk) / 4,  # rough token estimate\n",
        "                \"chunk_text\": chunk\n",
        "            })\n",
        "    return all_chunks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "d00afff3e1d7465882892cb0e5969bb5",
            "aa720ce307cb4b599e1b45b87bdc08ec",
            "ba780ecedf3345ac83afdb5e14431e62",
            "e79af3d79e194e44888788abfda48eb1",
            "4f228632586f45339fa94b6c8b4da336",
            "585f0f22fc2744dcaed32d960e9f6017",
            "b31f3c21b31c403ab404d27dd9febdc1",
            "90136238c0e949b0998ed2c7869451a3",
            "779d4f49dd774e58b2d235a23e807c66",
            "f5b297e5cbbb4ecb966693964e3234bf",
            "fca820e0eda6450188fcdd534ec11eda",
            "b4e4a62c3695459aacc4c83425efcafb",
            "17096fd9c57248ea9da7ac58628569b2",
            "24d9007f81314e60928ab1dc8bfa7e07",
            "abb580c373dc45a8aa9fd80d85606b41",
            "e9e59611e4fe4506bf288c121db18fda",
            "4f8253ef3a1043f6817fb3174b11a7d7",
            "ccf261a4cc984cc3be620cc9fe530bde",
            "ad0e0decbc9342c2857ace57d3ec7064",
            "9898faa5092a4f61b038392581ab0806",
            "1bd1e3cc27fe45cfbc9e6b8547d922f8",
            "c22bfd831867459db1af92d1bdf7a14a",
            "a1ce8b23bf8f4045ad1e2c3069a95685",
            "bc906dbcfb7c4388821341df87b7e38e",
            "28e039ed2c084163abe404d9293cab3f",
            "b37223e7076941328e1b3663af3592e6",
            "9b156edd693c4c189990fc9dee481aea",
            "a0935b508bb541c783f7a9832e4517a3",
            "b9bb76d73bf94da59cb525a96875982e",
            "7be3931c740246ce9614a5192d613153",
            "70b3c5677c844adeb42851f0ff3667ef",
            "4d8a9c7deb0245e092b7182d6ae4a6ea",
            "8b827b037ebc422aaa773b3794b6b0b3",
            "6f5b0a0d1e23478ba5600221fa723583",
            "487e60da3cf54f9e90acc915ad6562b3",
            "4998c593982843558fff277f81800b29",
            "4b9e6cc6fc24422a97012313682532e1",
            "1d2f118ec52641aa89a3586312c6cce4",
            "564ca536b0814b23a70ce0c2f3b5cf88",
            "e07f63b87a5940859744c150f6f11dc7",
            "5f965b974e8348dd8851f05c5b814183",
            "c14708cb81134dd0b0cbd82779622898",
            "4f8d39128f9e4aa5a6bc0e58c965746e",
            "2e62c17e00654e36829899d37f6cfdd2",
            "ad761d6600c24452a85d1ed066478c58",
            "7b7478eb6ed643e89177e75e80d4e908",
            "b5fd430bdae34dc692c3f2381962d1ca",
            "792e40deba384b978cbf1cedaefe9425",
            "2c00bc94278247198df984fe216ac0f4",
            "0125ba876bde487c8eedaf100f49f3a7",
            "5f18446c08294540bc6b30bc682edbc4",
            "de86ebc82dba4d838d4a1bdd5e19579e",
            "a9c9f385788845ba83f5e1ef85813c7f",
            "9e44071ca5ad4f70a0c31d62c0cf2746",
            "b60c437851834e9ab72ce2d609c0a63c",
            "e7c4ada25442422ca81b884e728e4639",
            "8525d98cad2e492baafdd095e8d32e95",
            "182065831d154727bb15bbc2b144c91e",
            "4922a09ec8344222b45899ab5fd43d44",
            "f9d8123d46ff4e9dbb720c737fab75d9",
            "482197e0c64b4bee99af5f8f030fde4c",
            "f568d1f541d14ceb8add4194c3d9cf0e",
            "1902afe336d44b03b0021cb8fef872a1",
            "e7a3daca6bf04243a5d433e555396c51",
            "5ecd6567fefa4d4e93ddc64d07f57791",
            "9140ac0ba44545ea97a53f26eb238112",
            "b1640adb9657487594884932d84ea667",
            "21b261b55ec8457e860a10f783e53966",
            "53f794cf43a44ba4af6401efc1433ab9",
            "fb54cb49c0df47c296dcd7a8a6550021",
            "23f9c6f22bcf48f193ec11f8e6c7d6a5",
            "314cdef725774ff5bee65adb6dacc509",
            "8b4007e44ebf45938a3c319338d4b196",
            "72a5ed8c0a534f5d9144245d2796d7bd",
            "c41af7508f9d4b4ab2fb7a377b896a7a",
            "2dc15b26e5954bd7b40ea606b91c0003",
            "490f43184d1a41faaa9e7a4a600dbfd7",
            "bf50d32e34964070bd6801dd9e092600",
            "fe5e48897aaa4b2b81f19ddcfd167404",
            "ff1a5b2da5dc4778a9e60cfb424cecfc",
            "201772fb56d74857ad4bbcaf5009b86d",
            "aa825b9a71844c01820a8c34594eacfe",
            "fb55b1318e6c47efb87fd5d91b1fa145",
            "787009170ef04db497fedef6971f45a6",
            "e862bc4367d54048844dbe0dd875c2f4",
            "03bf26db01374e28bf9daaa4101347b7",
            "c5263b31ae2d40479aba38a71115fe2b",
            "bc42cf80e3194db7833257c56e175847",
            "0a81df9c6d3b461b85ede4b43e1a044d",
            "f8fb788646244098b3cb1d64357f5bb3",
            "23177a768ee24a07bb70b3313760a70f",
            "29807d1ab6be40d589951bb474896e37",
            "129d1c6fade14ff3af97a4dc727b09ea",
            "2d15a402fbe64518ac46bf289ca0f8aa",
            "8e53b705e1614263ab791e8b1ab9bdaa",
            "07071243f2f04813b9f1f3c0ef6e0740",
            "a4dc76c012114715ab1a21bc684aed58",
            "04a4fdc71591425b8f09d643e492dae8",
            "bf3ae8d9b01943f0bc17a526cda553e3",
            "7761456e2bee4245802a0e4201113252",
            "d37971e13b1b470cbd5ac760997354c0",
            "fa56b4bbf877440eabd5c6f5dff084c5",
            "cc217b5845fa4499bfabb97598169526",
            "33a0c77dcb38407189e826574d775557",
            "7c71c6b3067e4c2aa8593db2ae796e6b",
            "5337eb0b9d434a848e2b84f9eb991752",
            "5f50d21c00444bf6b442d1e3c7774bdd",
            "f8696011c7724e2ca3c5785946456973",
            "ee2195b9fe1a4eee9ce36557fc9f6fd7",
            "fa74490bbaad4cc8a79768b334cfb22f",
            "2deddce42b874fde981d6497240599be",
            "089a8cb82f5f492396de3e8bb50473cc",
            "c643fbb9a7564411be65cf5dddbf8aea",
            "63a1e735c1e8452b847d2cfefc65742c",
            "6ca8cf62aa0a4edfab4f889bd8b283cd",
            "0aedfa1a3496446ea0ede8750e33ebc5",
            "15aa2c13ca1247f69bae38bc1a8ffd91",
            "96727aab380b4d48b01f75af6cd65042",
            "4c02bc7ec0074805b56e661b9fe502fa",
            "093b0a3f32404f82a289cc1575d8d00f",
            "1e1be9d6b22a40e8b50d3f31eaa903f4"
          ]
        },
        "id": "aCN1e3Ak-aUz",
        "outputId": "1cf3b2d1-7c79-4e72-d545-97c0692ae7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d00afff3e1d7465882892cb0e5969bb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4e4a62c3695459aacc4c83425efcafb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1ce8b23bf8f4045ad1e2c3069a95685"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f5b0a0d1e23478ba5600221fa723583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad761d6600c24452a85d1ed066478c58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7c4ada25442422ca81b884e728e4639"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1640adb9657487594884932d84ea667"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf50d32e34964070bd6801dd9e092600"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a81df9c6d3b461b85ede4b43e1a044d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7761456e2bee4245802a0e4201113252"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2deddce42b874fde981d6497240599be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "semantic_chunked_pages = semantic_chunk_pdf_pages(pages_and_texts,\n",
        "                                                  similarity_threshold=0.75,\n",
        "                                                  max_tokens=500)\n",
        "\n",
        "print(f\"Total semantic chunks: {len(semantic_chunked_pages)}\")\n",
        "print(f\"First semantic chunk (page {semantic_chunked_pages[0]['page_number']}):\")\n",
        "print(semantic_chunked_pages[0]['chunk_text'][:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "e3e25a004a2e4aff980ef6085da54090",
            "1f551cae366c4cc5974d7e700249797d",
            "a686a8f36c8f4789acae445d9b0c73d6",
            "b50ce6eaad6c432eb4d2d2ae695830aa",
            "bf688427bd204d8fbf3d7745643a8f6f",
            "1451fbbe14ee4eab8783d7d41cc19d5e",
            "965e114e340949a6a8079dca563b1d60",
            "fe1b5b480e5e4452aedeff199ae1dd19",
            "cdbcbb07a9f74c54916e75517b204806",
            "f3b4b350e8db496d8e9b29e3fd70dc92",
            "18cfb6854b0247c7838b613f2ecc6f51"
          ]
        },
        "id": "1ebP3ILT9fKp",
        "outputId": "e312e3fb-cf37-4b9c-8133-8495f2c2961f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3e25a004a2e4aff980ef6085da54090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Semantic chunking pages:   0%|          | 0/1208 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total semantic chunks: 12016\n",
            "First semantic chunk (page -41):\n",
            "Human Nutrition: 2020 Edition...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty-print 5 random SEMANTIC chunks (uses `semantic_chunked_pages` from your code above)\n",
        "\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
        "    \"\"\"Evenly spaced anchors + random jitter → indices scattered across [0, n-1].\"\"\"\n",
        "    if k <= 0:\n",
        "        return []\n",
        "    if k == 1:\n",
        "        return [random.randrange(n)]\n",
        "    anchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "    out, seen = [], set()\n",
        "    radius = max(1, int(n * jitter_frac))\n",
        "    for a in anchors:\n",
        "        lo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
        "        j = random.randint(lo, hi)\n",
        "        if j not in seen:\n",
        "            out.append(j); seen.add(j)\n",
        "    while len(out) < k:\n",
        "        r = random.randrange(n)\n",
        "        if r not in seen:\n",
        "            out.append(r); seen.add(r)\n",
        "    return out\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    approx_tokens = c.get('chunk_token_count', len(c.get('chunk_text', ''))/4)\n",
        "    header = (\n",
        "        f\" Chunk p{c['page_number']} · idx {c['chunk_index']}  |  \"\n",
        "        f\"chars {c['chunk_char_count']} · words {c['chunk_word_count']} · ~tokens {round(approx_tokens, 2)} \"\n",
        "    )\n",
        "    wrapped_lines = textwrap.wrap(\n",
        "        c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "    )\n",
        "    content_width = max([0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), content_width + 2)  # +2 for side padding\n",
        "\n",
        "    top    = \"╔\" + \"═\" * box_width + \"╗\"\n",
        "    hline  = \"║\" + header.ljust(box_width) + \"║\"\n",
        "    sep    = \"╟\" + \"─\" * box_width + \"╢\"\n",
        "    body   = \"\\n\".join(\"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or \\\n",
        "             (\"║ \" + \"\".ljust(box_width - 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"═\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "\n",
        "def show_random_semantic_chunks(semantic_chunked_pages: list[dict], k: int = 5, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "    n = len(semantic_chunked_pages)\n",
        "    if n == 0:\n",
        "        print(\"No semantic chunks to display.\");\n",
        "        return\n",
        "    idxs = _scattered_indices(n, k)\n",
        "    print(f\"Showing {len(idxs)} scattered random SEMANTIC chunks out of {n} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(semantic_chunked_pages[idx]))\n",
        "        print()\n",
        "\n",
        "# --- Run (expects you've already created `semantic_chunked_pages`) ---\n",
        "assert 'semantic_chunked_pages' in globals() and len(semantic_chunked_pages) > 0, \\\n",
        "    \"Run your semantic chunking code first to define `semantic_chunked_pages`.\"\n",
        "show_random_semantic_chunks(semantic_chunked_pages, k=5, seed=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg72I3lXCsKB",
        "outputId": "21b0afbb-ec40-4b0d-c1a1-eaac5420e7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing 5 scattered random SEMANTIC chunks out of 12016 total:\n",
            "\n",
            "#1\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p56 · idx 7  |  chars 200 · words 31 · ~tokens 50.0                                    ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Observing the connection  between the beverage and longevity, Dr. Elie Metchnikoff began his ║\n",
            "║ research on beneficial bacteria and the longevity of life that led to  his book, The         ║\n",
            "║ Prolongation of Life.                                                                        ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#2\n",
            "╔════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p232 · idx 7  |  chars 82 · words 12 · ~tokens 20.5                          ║\n",
            "╟────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ This small  structural alteration causes galactose to be less stable than glucose. ║\n",
            "╚════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#3\n",
            "╔═════════════════════════════════════════════════════════╗\n",
            "║ Chunk p509 · idx 9  |  chars 30 · words 6 · ~tokens 7.5 ║\n",
            "╟─────────────────────────────────────────────────────────╢\n",
            "║ Rodearmel SJ, Wyatt HR, et al.                          ║\n",
            "╚═════════════════════════════════════════════════════════╝\n",
            "\n",
            "#4\n",
            "╔══════════════════════════════════════════════════════════╗\n",
            "║ Chunk p962 · idx 17  |  chars 24 · words 4 · ~tokens 6.0 ║\n",
            "╟──────────────────────────────────────────────────────────╢\n",
            "║ 962  |  Sports Nutrition                                 ║\n",
            "╚══════════════════════════════════════════════════════════╝\n",
            "\n",
            "#5\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p1110 · idx 5  |  chars 78 · words 13 · ~tokens 19.5                     ║\n",
            "╟────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Instead,  cells use fat and proteins to make energy, resulting in weight loss. ║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def recursive_chunk_text(text: str, max_chunk_size: int = 1000, min_chunk_size: int = 100) -> list:\n",
        "    \"\"\"\n",
        "    Recursively splits a block of text into chunks that fit within size constraints.\n",
        "    Tries splitting by sections, then newlines, then sentences.\n",
        "    \"\"\"\n",
        "    def split_chunk(chunk: str) -> list:\n",
        "        #  Base case\n",
        "        if len(chunk) <= max_chunk_size:\n",
        "            return [chunk]\n",
        "\n",
        "        #  Try splitting by double newlines\n",
        "        sections = chunk.split(\"\\n\\n\")\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "\n",
        "        # Try splitting by single newline\n",
        "        sections = chunk.split(\"\\n\")\n",
        "        if len(sections) > 1:\n",
        "            result = []\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    result.extend(split_chunk(section.strip()))\n",
        "            return result\n",
        "\n",
        "        # Fallback: split by sentences\n",
        "        sentences = nltk.sent_tokenize(chunk)\n",
        "        chunks, current_chunk, current_size = [], [], 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if current_size + len(sentence) > max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_size = len(sentence)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += len(sentence)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    return split_chunk(text)\n",
        "\n",
        "\n",
        "def recursive_chunk_pdf_pages(pages_and_texts: list,\n",
        "                              max_chunk_size: int = 1000,\n",
        "                              min_chunk_size: int = 100) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Takes PDF pages with text and splits them into recursive chunks.\n",
        "\n",
        "    Returns a list of dicts with page_number, chunk_index, and chunk_text.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    for page in tqdm(pages_and_texts, desc=\"Recursive chunking pages\"):\n",
        "        page_number = page[\"page_number\"]\n",
        "        page_text = page[\"text\"]\n",
        "\n",
        "        chunks = recursive_chunk_text(page_text,\n",
        "                                      max_chunk_size=max_chunk_size,\n",
        "                                      min_chunk_size=min_chunk_size)\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"page_number\": page_number,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_char_count\": len(chunk),\n",
        "                \"chunk_word_count\": len(chunk.split()),\n",
        "                \"chunk_token_count\": len(chunk) / 4,  # rough token estimate\n",
        "                \"chunk_text\": chunk\n",
        "            })\n",
        "    return all_chunks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyf3lzlqEM42",
        "outputId": "843bcd05-2c67-48c5-c3bd-830d102d95c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_chunked_pages = recursive_chunk_pdf_pages(pages_and_texts,\n",
        "                                                    max_chunk_size=800,\n",
        "                                                    min_chunk_size=100)\n",
        "\n",
        "print(f\"Total recursive chunks: {len(recursive_chunked_pages)}\")\n",
        "print(f\"First recursive chunk (page {recursive_chunked_pages[0]['page_number']}):\")\n",
        "print(recursive_chunked_pages[0]['chunk_text'][:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "d80c13f6a34a4aabb7a255f9ea3cc427",
            "577e4c910cae45caba1f51955835e74a",
            "50200d74931b4cbdbbd2bc591d880101",
            "a3977f9a8ee746b6a417fb77ccff9ae3",
            "aea48c11d23d4302b9f3f63aecf6df1e",
            "eaba599369dd4b638d771e64f8332b12",
            "de0050fac48e449e90ae11beeb117652",
            "da6a2dac6bff4e25a6716d0d34e3010b",
            "73427434ea9b49669e088df2b4b9e91c",
            "1e9a348a7a7a4fe2baa01f7ac1fdc390",
            "e70e8b2e7fe541f987e88708592abfe6"
          ]
        },
        "id": "T-Vi4AWOH04V",
        "outputId": "48943d0a-5f18-4311-afd2-f8d2dc2f1fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Recursive chunking pages:   0%|          | 0/1208 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d80c13f6a34a4aabb7a255f9ea3cc427"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total recursive chunks: 2434\n",
            "First recursive chunk (page -41):\n",
            "Human Nutrition: 2020 Edition...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty-print 5 random RECURSIVE chunks (uses `recursive_chunked_pages` from your code above)\n",
        "\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
        "    \"\"\"Evenly spaced anchors + random jitter → indices scattered across [0, n-1].\"\"\"\n",
        "    if k <= 0:\n",
        "        return []\n",
        "    if k == 1:\n",
        "        return [random.randrange(n)]\n",
        "    anchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
        "    out, seen = [], set()\n",
        "    radius = max(1, int(n * jitter_frac))\n",
        "    for a in anchors:\n",
        "        lo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
        "        j = random.randint(lo, hi)\n",
        "        if j not in seen:\n",
        "            out.append(j); seen.add(j)\n",
        "    while len(out) < k:\n",
        "        r = random.randrange(n)\n",
        "        if r not in seen:\n",
        "            out.append(r); seen.add(r)\n",
        "    return out\n",
        "\n",
        "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
        "    approx_tokens = c.get('chunk_token_count', len(c.get('chunk_text', '')) / 4)\n",
        "    header = (\n",
        "        f\" Chunk p{c['page_number']} · idx {c['chunk_index']}  |  \"\n",
        "        f\"chars {c['chunk_char_count']} · words {c['chunk_word_count']} · ~tokens {round(approx_tokens, 2)} \"\n",
        "    )\n",
        "    wrapped_lines = textwrap.wrap(\n",
        "        c[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
        "    )\n",
        "    content_width = max([0, *map(len, wrapped_lines)])\n",
        "    box_width = max(len(header), content_width + 2)  # +2 for side padding\n",
        "\n",
        "    top    = \"╔\" + \"═\" * box_width + \"╗\"\n",
        "    hline  = \"║\" + header.ljust(box_width) + \"║\"\n",
        "    sep    = \"╟\" + \"─\" * box_width + \"╢\"\n",
        "    body   = \"\\n\".join(\"║ \" + line.ljust(box_width - 2) + \" ║\" for line in wrapped_lines) or \\\n",
        "             (\"║ \" + \"\".ljust(box_width - 2) + \" ║\")\n",
        "    bottom = \"╚\" + \"═\" * box_width + \"╝\"\n",
        "    return \"\\n\".join([top, hline, sep, body, bottom])\n",
        "\n",
        "def show_random_recursive_chunks(recursive_chunked_pages: list[dict], k: int = 5, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "    n = len(recursive_chunked_pages)\n",
        "    assert n > 0, \"No recursive chunks to display. Did you run the recursive chunking cell?\"\n",
        "    idxs = _scattered_indices(n, k)\n",
        "    print(f\"Showing {len(idxs)} scattered random RECURSIVE chunks out of {n} total:\\n\")\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        print(f\"#{i}\")\n",
        "        print(_draw_boxed_chunk(recursive_chunked_pages[idx]))\n",
        "        print()\n",
        "\n",
        "# --- Run (expects you've already created `recursive_chunked_pages`) ---\n",
        "assert 'recursive_chunked_pages' in globals() and len(recursive_chunked_pages) > 0, \\\n",
        "    \"Run your recursive chunking code first to define `recursive_chunked_pages`.\"\n",
        "show_random_recursive_chunks(recursive_chunked_pages, k=5, seed=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnAbTCuxIYIG",
        "outputId": "6bafca04-dbd2-46c6-e9e7-fd1fffff3540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing 5 scattered random RECURSIVE chunks out of 2434 total:\n",
            "\n",
            "#1\n",
            "╔═══════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p53 · idx 0  |  chars 69 · words 14 · ~tokens 17.25             ║\n",
            "╟───────────────────────────────────────────────────────────────────────╢\n",
            "║ PART II  CHAPTER 2. THE HUMAN  BODY  Chapter 2. The Human Body  |  53 ║\n",
            "╚═══════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#2\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p213 · idx 0  |  chars 761 · words 102 · ~tokens 190.25                                    ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Deadly  water-borne  illnesses  decreased to  almost  nonexistent  levels in the  United  States ║\n",
            "║ after  the  implementat ion of water  disinfection  methods. public water systems in the country ║\n",
            "║ adhere to the standards. About  15 percent of Americans obtain drinking water from private       ║\n",
            "║ wells,  which are not subject to EPA standards. Producing water safe for drinking involves some  ║\n",
            "║ or all of the  following processes: screening out large objects, removing excess  calcium        ║\n",
            "║ carbonate from hard water sources, flocculation, which  adds a precipitating agent to remove     ║\n",
            "║ solid particles, clarification,  sedimentation, filtration, and disinfection. These processes    ║\n",
            "║ aim to  remove unhealthy substances and produce high-quality, colorless,  odorless, good-tasting ║\n",
            "║ water.                                                                                           ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#3\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p480 · idx 1  |  chars 780 · words 123 · ~tokens 195.0                                     ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Total Energy Expenditure (Output)  The amount of energy you expend every day includes not only   ║\n",
            "║ the  calories you burn during physical activity, but also the calories you  burn while at rest   ║\n",
            "║ (basal metabolism), and the calories you burn  when you digest food. The sum of caloric          ║\n",
            "║ expenditure is referred  to as total energy expenditure (TEE). Basal metabolism refers to  those ║\n",
            "║ metabolic pathways necessary to support and maintain the  body’s basic functions (e.g.           ║\n",
            "║ breathing, heartbeat, liver and kidney  function) while at rest. The basal metabolic rate (BMR)  ║\n",
            "║ is the amount  of energy required by the body to conduct its basic functions over  a certain     ║\n",
            "║ time period. The great majority of energy expended  (between 50 and 70 percent) daily is from    ║\n",
            "║ conducting life’s basic  processes.                                                              ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#4\n",
            "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p972 · idx 0  |  chars 768 · words 116 · ~tokens 192.0                                     ║\n",
            "╟──────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Image by  Allison  Calabrese /  CC BY 4.0  Water and Electrolyte Needs  UNIVERSITY OF HAWAI‘I AT ║\n",
            "║ MĀNOA FOOD SCIENCE AND HUMAN  NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM  During exercise,    ║\n",
            "║ being appropriately hydrated contributes to  performance. Water is needed to cool the body,      ║\n",
            "║ transport oxygen  and nutrients, and remove waste products from the muscles. Water  needs are    ║\n",
            "║ increased during exercise due to the extra water losses  through evaporation and sweat.          ║\n",
            "║ Dehydration can occur when there  is inadequate water levels in the body and can be very         ║\n",
            "║ hazardous to  the health of an individual. As the severity of dehydration increases,  the        ║\n",
            "║ exercise performance of an individual will begin to decline (see  Figure 16.9 “Dehydration       ║\n",
            "║ Effect on Exercise Performance”).                                                                ║\n",
            "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "#5\n",
            "╔═════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "║ Chunk p1111 · idx 0  |  chars 721 · words 124 · ~tokens 180.25                                  ║\n",
            "╟─────────────────────────────────────────────────────────────────────────────────────────────────╢\n",
            "║ Image by  Allison  Calabrese /  CC BY 4.0  Type 2 Diabetes  The other 90 to 95 percent of       ║\n",
            "║ diabetes cases are Type 2 diabetes.  Type 2 diabetes is defined as a metabolic disease of       ║\n",
            "║ insulin  insufficiency, but it is also caused by muscle, liver, and fat cells no  longer        ║\n",
            "║ responding to the insulin in the body (Figure 18.4 “Healthy  Individuals and Type 2 Diabetes” . ║\n",
            "║ In brief, cells in the body have  become resistant to insulin and no longer receive the full    ║\n",
            "║ physiological message of insulin to take up glucose from the blood.  Thus, similar to patients  ║\n",
            "║ with Type 1 diabetes, those with Type 2  diabetes also have high blood-glucose levels.  Figure  ║\n",
            "║ 18.4 Healthy Individuals and Type 2 Diabetes  Threats to Health  |  1111                        ║\n",
            "╚═════════════════════════════════════════════════════════════════════════════════════════════════╝\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "# 1) Helper to detect \"chapter start\" pages\n",
        "def _is_chapter_header_page(text: str) -> bool:\n",
        "    # Robust to punctuation/diacritics differences; matches the recurring header\n",
        "    # e.g., \"UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM\"\n",
        "    return re.search(r\"university\\s+of\\s+hawai\", text, flags=re.IGNORECASE) is not None\n",
        "\n",
        "def _guess_title_from_page(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Best-effort chapter title guess = the text before the 'University of Hawai' header line.\n",
        "    Falls back to the first ~120 characters.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"university\\s+of\\s+hawai\", text, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        title = text[:m.start()].strip()\n",
        "        # keep it readable\n",
        "        title = re.sub(r\"\\s+\", \" \", title).strip()\n",
        "        if 10 <= len(title) <= 180:\n",
        "            return title\n",
        "    # fallback\n",
        "    t = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return t[:120] if t else \"Untitled Chapter\"\n",
        "\n",
        "# 2) Build chapter chunks\n",
        "def chapter_chunk_pdf_pages(pages_and_texts: list[dict]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Returns a list of chapter chunks:\n",
        "    [\n",
        "      {\n",
        "        'chapter_index': int,\n",
        "        'title': str,\n",
        "        'page_start': int,   # adjusted page number (your -41 offset)\n",
        "        'page_end': int,\n",
        "        'chunk_char_count': int,\n",
        "        'chunk_word_count': int,\n",
        "        'chunk_token_count': float,   # ~chars/4\n",
        "        'chunk_text': str\n",
        "      }, ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    if not pages_and_texts:\n",
        "        return []\n",
        "\n",
        "    # Find all page indices that look like the start of a chapter\n",
        "    chapter_starts = []\n",
        "    for i, p in enumerate(pages_and_texts):\n",
        "        txt = p[\"text\"]\n",
        "        if _is_chapter_header_page(txt):\n",
        "            chapter_starts.append(i)\n",
        "\n",
        "    # If nothing detected, return empty (or treat entire doc as one chunk)\n",
        "    if not chapter_starts:\n",
        "        # Treat entire doc as one \"chapter\"\n",
        "        all_text = \" \".join(p[\"text\"] for p in pages_and_texts).strip()\n",
        "        return [{\n",
        "            \"chapter_index\": 0,\n",
        "            \"title\": _guess_title_from_page(pages_and_texts[0][\"text\"]),\n",
        "            \"page_start\": pages_and_texts[0][\"page_number\"],\n",
        "            \"page_end\": pages_and_texts[-1][\"page_number\"],\n",
        "            \"chunk_char_count\": len(all_text),\n",
        "            \"chunk_word_count\": len(all_text.split()),\n",
        "            \"chunk_token_count\": round(len(all_text) / 4, 2),\n",
        "            \"chunk_text\": all_text\n",
        "        }]\n",
        "\n",
        "    # Build chapter ranges (start -> next_start-1)\n",
        "    chapter_chunks = []\n",
        "    for ci, s in enumerate(chapter_starts):\n",
        "        e = (chapter_starts[ci + 1] - 1) if (ci + 1 < len(chapter_starts)) else (len(pages_and_texts) - 1)\n",
        "        if e < s:\n",
        "            continue  # guard (shouldn't happen)\n",
        "\n",
        "        pages = pages_and_texts[s:e + 1]\n",
        "        text_concat = \" \".join(p[\"text\"] for p in pages).strip()\n",
        "        title = _guess_title_from_page(pages[0][\"text\"])\n",
        "\n",
        "        chapter_chunks.append({\n",
        "            \"chapter_index\": ci,\n",
        "            \"title\": title,\n",
        "            \"page_start\": pages[0][\"page_number\"],\n",
        "            \"page_end\": pages[-1][\"page_number\"],\n",
        "            \"chunk_char_count\": len(text_concat),\n",
        "            \"chunk_word_count\": len(text_concat.split()),\n",
        "            \"chunk_token_count\": round(len(text_concat) / 4, 2),\n",
        "            \"chunk_text\": text_concat\n",
        "        })\n",
        "\n",
        "    return chapter_chunks"
      ],
      "metadata": {
        "id": "uZ3dxWdiJvp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structure_chunked_pages = chapter_chunk_pdf_pages(pages_and_texts)\n",
        "\n",
        "print(f\"Total chapter-based chunks: {len(structure_chunked_pages)}\")\n",
        "if structure_chunked_pages:\n",
        "    first = structure_chunked_pages[0]\n",
        "    print(f\"First chapter (pages {first['page_start']}–{first['page_end']}): {first['title']}\")\n",
        "    print(first['chunk_text'][:200] + \"...\")\n",
        "else:\n",
        "    print(\"No chapters detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5Ycgsg2eL8A",
        "outputId": "a0330550-9c9d-4f87-cf43-c71ed54517ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chapter-based chunks: 171\n",
            "First chapter (pages -39–-39): Human Nutrition: 2020 Edition\n",
            "Human Nutrition: 2020  Edition  UNIVERSITY OF HAWAI‘I AT MĀNOA  FOOD SCIENCE AND HUMAN  NUTRITION PROGRAM  ALAN TITCHENAL, SKYLAR HARA,  NOEMI ARCEO CAACBAY, WILLIAM  MEINKE-LAU, YA-YUN YANG, MARIE  K...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from ipywidgets import Widget\n",
        "    # This closes all active widget instances (like progress bars)\n",
        "    # effectively clearing the \"state\" metadata causing the error.\n",
        "    Widget.close_all()\n",
        "    print(\"Success: All widget states have been cleared.\")\n",
        "except ImportError:\n",
        "    print(\"No widgets library found, so no widgets to clear.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTQqbpdSeQ3j",
        "outputId": "024d869b-c542-41f1-9b3f-41a19caf9196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: All widget states have been cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSOMV6y6RYF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}